{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jos neki resursi\n",
    "\n",
    "- [ ] [gsitechnology.com - A Beginnerâ€™s Guide To Segmentation In Satellite Images: Walking Through Machine Learning Techniques For Image Segmentation And Applying Them To Satellite Imagery](https://www.gsitechnology.com/Beginners-Guide-to-Segmentation-in-Satellite-Images)\n",
    "\n",
    "\n",
    "- [fegu856.com - Getting started with PyTorch: 1- Linear regression](http://www.fregu856.com/post/19apr/)\n",
    "\n",
    "- [freecodecamp - What Is a Convolutional Neural Network? A Beginner's Tutorial for Machine Learning and Deep Learning](https://www.freecodecamp.org/news/convolutional-neural-network-tutorial-for-beginners/)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "#### za ispitat\n",
    "\n",
    "- [ ] [kaggle.com - Segmentation in PyTorch using convenient tools](https://www.kaggle.com/artgor/segmentation-in-pytorch-using-convenient-tools)   \n",
    "    - understanding clouds from satelite images\n",
    "    - nisam siguran za ovaj \n",
    "- [ ] [towardsdatascience.com - Semantic hand segmentation using Pytorch](https://towardsdatascience.com/semantic-hand-segmentation-using-pytorch-3e7a0a0386fa)\n",
    "    - harder than **object detection**\n",
    "    - simpler than instance segmentation\n",
    "    \n",
    "    \n",
    "- github 2 projekta\n",
    "     - [torchsat](https://github.com/sshuair/torchsat)\n",
    "         - python, docker\n",
    "     - [satellite-building-detection.pytorch](https://github.com/YuansongFeng/satellite-building-detection.pytorch)\n",
    "         - archived\n",
    "         - jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic segmentation - pytorch\n",
    "\n",
    "\n",
    "- [PyTorch for Beginners: Semantic Segmentation using torchvision](https://learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/)\n",
    "    - [pytorch docs - FCN-RESNET101](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/)\n",
    "    - [RESNET](https://pytorch.org/hub/pytorch_vision_resnet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we group pixels based on color, texture, or other low level primitives, we call these perceptual groups **superpixels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# a pretrained model of FCN\n",
    "# pretrained=True - download model if not already present\n",
    "fcn = models.segmentation.fcn_resnet101(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "img = Image.open('./img/bird.jpg')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Pre-process the image\n",
    "\n",
    "uh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformations needed\n",
    "import torchvision.transforms as T\n",
    "trf = T.Compose([T.Resize(256),\n",
    "                 T.CenterCrop(300),  # na 300 je ok\n",
    "                 T.ToTensor(), \n",
    "                 T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                             std = [0.229, 0.224, 0.225])])\n",
    "inp = trf(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input through the net\n",
    "out = fcn(inp)['out']\n",
    "print (out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([1, 21, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n",
    "print(om.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.unique(om))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above is a 2D image where each pixel corresponds to a class\n",
    "\n",
    "and now, it's tiime. the last thing: take this 2D img and convert it into a segmentation map where each class label is converted into an RGB color\n",
    "\n",
    "### Decode Output\n",
    "\n",
    "the following function does that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the helper function\n",
    "def decode_segmap(image, nc=21):\n",
    "  \n",
    "  label_colors = np.array([(0, 0, 0),  # 0=background\n",
    "               # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
    "               (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 204, 204), (128, 0, 128),\n",
    "               # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
    "               (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n",
    "               # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n",
    "               (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n",
    "               # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "               (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])\n",
    "\n",
    "  r = np.zeros_like(image).astype(np.uint8)\n",
    "  g = np.zeros_like(image).astype(np.uint8)\n",
    "  b = np.zeros_like(image).astype(np.uint8)\n",
    "  \n",
    "  for l in range(0, nc):\n",
    "    idx = image == l\n",
    "    r[idx] = label_colors[l, 0]\n",
    "    g[idx] = label_colors[l, 1]\n",
    "    b[idx] = label_colors[l, 2]\n",
    "    \n",
    "  rgb = np.stack([r, g, b], axis=2)\n",
    "  return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = decode_segmap(om)\n",
    "plt.imshow(rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(net, path):\n",
    "  img = Image.open(path)\n",
    "  plt.imshow(img); plt.axis('off'); plt.show()\n",
    "  # Comment the Resize and CenterCrop for better inference results\n",
    "  trf = T.Compose([T.Resize(256), \n",
    "                   T.CenterCrop(400), \n",
    "                   T.ToTensor(), \n",
    "                   T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                               std = [0.229, 0.224, 0.225])])\n",
    "  inp = trf(img).unsqueeze(0)\n",
    "  out = net(inp)['out']\n",
    "  om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n",
    "  rgb = decode_segmap(om)\n",
    "  plt.imshow(rgb); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment(fcn, './img/white-tailed_Tropicbird.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment(fcn, './img/bicycle.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "miracles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLab is supposedly better\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<!-- - ![pre_trained_model_accuracy](pre_trained_model_accuracy.png) -->\n",
    "- [TORCHVISION.MODELS](https://pytorch.org/docs/stable/torchvision/models.html?highlight=faster%20rcnn)\n",
    "![segmentation models: FCN101 vs DeepLabV3 ResNet101](./img/segmentation_models.png)\n",
    "\n",
    "\n",
    "- [pyimagesearch.com - IoU](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) - intersection over union\n",
    "    - measure the accuracy of an object detector on a particular dataset\n",
    "    - good grade is at 0.73, allegedly\n",
    "    - poor 0.4\n",
    "    - excellent 0.93\n",
    "    - ![calculate IoU](./img/IoU.png)\n",
    "    \n",
    "<br>\n",
    "\n",
    "- [blog fergu856.com - PyTorch Implementation of DeepLabV3](http://www.fregu856.com/project/deeplabv3/)\n",
    "    - autonomus driving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlab = models.segmentation.deeplabv3_resnet101(pretrained=1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment(dlab, './img/bicycle.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe the images were too difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment(dlab, './img/boat.jfif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
